---
apiVersion: v1
kind: Service
metadata:
  name: mistral-gpu
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/app-metrics: "true"
    prometheus.io/port: "8080"
    service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip  
    service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing    
  labels:
    model: mistral7b-gpu
spec:
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 8080
  selector:
    model: mistral7b-gpu
  type: LoadBalancer

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mistral-gpu
  labels:
    model: mistral7b-gpu
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      model: mistral7b-gpu
  template:
    metadata:
      labels:
        model: mistral7b-gpu
      annotations:
        # Performance monitoring annotations
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"
    spec:
      # QoS Class: Guaranteed (requests = limits for critical resources)
      nodeSelector:
        # GPU node selection - adjust based on your GPU instance types
        node.kubernetes.io/instance-type: g5.2xlarge  # or g4dn.2xlarge, p3.2xlarge, etc.
        eks.amazonaws.com/compute-type: auto        
      tolerations:
        # GPU-specific tolerations
        - effect: NoSchedule
          key: nvidia.com/gpu
          operator: Exists
      initContainers:
        - name: model-download
          image: amazon/aws-cli
          command: ["/bin/bash", "-c"]
          args:
            - |
              set -e              
              echo "Checking target directory..."
              if [ ! -d "/tmp/models/mistral-7b-v0-2" ]; then
                echo "Creating target directory..."
                mkdir -p /tmp/models/mistral-7b-v0-2
              
                echo "Copying files..."
                if [ -d "/models/models/mistral-7b" ]; then
                  cp -rv /models/models/mistral-7b/* /tmp/models/mistral-7b-v0-2/
                  echo "Copy completed. New directory structure:"
                  rm -rf /tmp/models/mistral-7b-v0-2/compiled
                else
                  echo "Source directory /models/models/mistral-7b does not exist!"
                  exit 1
                fi
              else
                echo "Target directory already exists"
              fi
          volumeMounts:
            - name: model-volume
              mountPath: /models
            - name: local-storage
              mountPath: /tmp/models
          resources:
            requests:
              cpu: 100m
              memory: 256Mi
            limits:
              cpu: 500m
              memory: 1Gi
      containers:
        - name: vllm
          # Use official vLLM image with CUDA support
          image: vllm/vllm-openai:latest
          imagePullPolicy: IfNotPresent
          command: ["/bin/sh", "-c"]
          args: [
            # OPTIMIZED vLLM COMMAND for GPU with all recommended parameters
            "vllm serve /tmp/models/mistral-7b-v0-2 --tokenizer /tmp/models/mistral-7b-v0-2 --port 8080 --host 0.0.0.0 --device cuda --tensor-parallel-size 1 --max-num-seqs 16 --block-size 16 --use-v2-block-manager --max-model-len 2048 --dtype bfloat16 --swap-space 4 --max-paddings 256 --gpu-memory-utilization 0.9"
          ]
          ports:
            - containerPort: 8080
              protocol: TCP
              name: http
          # OPTIMIZED RESOURCES for GPU deployment
          resources:
            requests:
              cpu: 8               # Reduced CPU for GPU workload
              memory: 16Gi         # Reduced memory as GPU handles model
              nvidia.com/gpu: "1"  # Single GPU request
            limits:
              cpu: 10              # Slightly above requests for burst capacity
              memory: 24Gi         # Reasonable limit with headroom
              nvidia.com/gpu: "1"  # Single GPU limit
          # HEALTH CHECKS - Critical for production
          readinessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 60    # Allow time for model loading
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
          livenessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 120   # Allow more time for initial startup
            periodSeconds: 30
            timeoutSeconds: 10
            failureThreshold: 3
          # STARTUP PROBE - For slow-starting containers
          startupProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 18       # 3 minutes total startup time
          volumeMounts:
            - name: dshm
              mountPath: /dev/shm
            - name: local-storage
              mountPath: /tmp/models
            - name: model-volume
              mountPath: /models
          env:
            # CUDA/GPU CONFIGURATION
            - name: CUDA_VISIBLE_DEVICES
              value: "0"  # Use first GPU
            - name: NVIDIA_VISIBLE_DEVICES
              value: "all"
            - name: NVIDIA_DRIVER_CAPABILITIES
              value: "compute,utility"
            # VLLM CONFIGURATION
            - name: VLLM_LOGGING_LEVEL
              value: "INFO"
            - name: VLLM_ENGINE_ITERATION_TIMEOUT_S
              value: "60"
            # PERFORMANCE OPTIMIZATION
            - name: VLLM_WORKER_MULTIPROC_METHOD
              value: "spawn"
            - name: PYTORCH_CUDA_ALLOC_CONF
              value: "max_split_size_mb:128"
            # MEMORY OPTIMIZATION
            - name: MALLOC_ARENA_MAX
              value: "4"
            - name: MALLOC_MMAP_THRESHOLD_
              value: "131072"
            # GPU MEMORY OPTIMIZATION
            - name: CUDA_LAUNCH_BLOCKING
              value: "0"  # Async CUDA operations
            - name: CUDA_CACHE_DISABLE
              value: "0"  # Enable CUDA cache
      # GRACEFUL SHUTDOWN
      terminationGracePeriodSeconds: 30  # Increased for proper cleanup
      volumes:
        - name: dshm
          emptyDir:
            medium: Memory
            sizeLimit: 8Gi  # INCREASED shared memory for higher concurrency
        - name: model-volume
          persistentVolumeClaim:
            claimName: fsx-models
        - name: local-storage
          emptyDir:
            sizeLimit: 50Gi  # Limit local storage usage

---
# OPTIONAL: Resource Quota for namespace-level resource management
apiVersion: v1
kind: ResourceQuota
metadata:
  name: mistral-gpu-quota
spec:
  hard:
    requests.cpu: "12"
    requests.memory: 24Gi
    limits.cpu: "16"
    limits.memory: 32Gi
    nvidia.com/gpu: "2"

---
# OPTIONAL: Pod Disruption Budget for availability
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: mistral-gpu-pdb
spec:
  minAvailable: 1
  selector:
    matchLabels:
      model: mistral7b-gpu
